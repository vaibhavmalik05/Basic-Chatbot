{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\H263429\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\H263429\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\H263429\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the .txt file using a context manager\n",
    "with open(\"Corpus.txt\", \"r\") as file:\n",
    "    Corpus = file.read()\n",
    "    # print(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string.punctuation attribute contains all the punctuation characters. We use the str.maketrans() method to create a translation table that maps each punctuation character to None, effectively removing them from the text. The translate() method then applies the translation table to the text, removing the punctuation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation and tokenize the words\n",
    "This is not included in the function process below so we can update word tokens and sentence tokens based on the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the punctuation marks to retain\n",
    "punctuation_to_retain = \",!\"\n",
    "\n",
    "# Create a translation table with punctuation marks to remove\n",
    "translator = str.maketrans('', '', string.punctuation.replace(punctuation_to_retain, ''))\n",
    "\n",
    "# Remove punctuation marks except the ones to retain\n",
    "text_wo_punc = Corpus.translate(translator)\n",
    "global word_token, sent_tokens\n",
    "word_tokens = nltk.word_tokenize(text_wo_punc)\n",
    "sent_tokens = nltk.sent_tokenize(Corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the stop words and Perform Lemmatization\n",
    "\n",
    "def process(Corpus):\n",
    "\n",
    "    # Lowering, stopwords/punctuation removal, tokeinzing\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    corp = [word.lower() for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the text\n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "\n",
    "    lem_corp = [lemmatizer.lemmatize(word) for word in corp]\n",
    "\n",
    "    return lem_corp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to greet our user with Hi, Hello\n",
    "\n",
    "Input_greet = [\"hi\", \"hello\", \"sup\", \"what's up\", \"greetings\", \"hey\", \"hi,\", \"hello,\", \"sup,\", \"what's up,\", \"greetings!\", \"hey,\"]\n",
    "output_greet = ['Hi', \"Hey there!\", \"I am glad you are talking to me\", \"Hey, I am here to help\"]\n",
    "\n",
    "def greet(sentence):\n",
    "    return [random.choice(output_greet) for word in sentence.split(\" \") if word.lower() in Input_greet][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the response based on user input and perform tfidf on the data\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "n_not_und = 0   # Variable that is taken to set the limit of number of attempts where our bot does not know the answer\n",
    "\n",
    "def response(user_response):\n",
    "    robo1_response=''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = process, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)   ## Determining similarity of last index w.r.t. others\n",
    "    idx = vals.argsort()[0][-2]  # retrieves the index of the document that has the second highest cosine similarity value with the last document in the valsarray. (Here [0] gives the array of indices (because our aim to find the index here) and [-2] determines of which location we have to find indices (i.e. secodn last))\n",
    "    req_tfidf = np.sort(vals[0])[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        robo1_response = robo1_response + \"I am sorry! I don't understand you\"\n",
    "        n_not_und = n_not_und + 1\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response + sent_tokens[idx]\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am Bot Vaibhav Malik, How I can help you, In case you need to quit just type Bye, or thank you.\n",
      "Vaibhav:  Hi\n",
      "Vaibhav:  Both fields play vital roles in leveraging the power of data to understand patterns, make informed decisions, and solve complex problems across various domains.\n",
      "Vaibhav:  [6][26]\n",
      "\n",
      "Modern usage\n",
      "In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[27] a catchphrase that was picked up even by major-city newspapers like the New York Times[28] and the Boston Globe.\n",
      "Vaibhav:  In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.)\n",
      "Vaibhav: Thank you for Contacting me, Please contact again. \n"
     ]
    }
   ],
   "source": [
    "# Function to interact with the client\n",
    "\n",
    "Status =True\n",
    "print(\"Hello I am Bot Vaibhav Malik, How I can help you, In case you need to quit just type Bye, or thank you.\")\n",
    "while Status:\n",
    "    user_response = input().strip()\n",
    "    user_response = user_response.lower()\n",
    "    if user_response in ['bye', 'thanks', 'thank you']:\n",
    "        Status = False\n",
    "        print (\"Vaibhav: Thank you for Contacting me, Please contact again. \")\n",
    "\n",
    "    else:\n",
    "        if user_response in Input_greet:\n",
    "            print(\"Vaibhav: \", greet(user_response))\n",
    "\n",
    "        else:\n",
    "            sent_tokens.append(user_response)\n",
    "            word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "            print(\"Vaibhav: \", response(user_response))\n",
    "            if (n_not_und >= 3):\n",
    "                print( \"Vaibhav: Sorry I am leaving, I am unable to understand your query \")\n",
    "                Status = False\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
